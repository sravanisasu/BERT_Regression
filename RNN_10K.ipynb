{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN-10K.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcNM9YX4PfDoousdPd+0ps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravanisasu/BERT_Regression/blob/main/RNN_10K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYcjLMdXn1vk"
      },
      "source": [
        "**git clone dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Erh5bOtmg_1V",
        "outputId": "f054716b-8fc4-493a-b583-0542648a7106"
      },
      "source": [
        "!git clone https://github.com/sravanisasu/10k-sample"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '10k-sample'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 9557 (delta 1), reused 1 (delta 0), pack-reused 9548\u001b[K\n",
            "Receiving objects: 100% (9557/9557), 158.15 MiB | 20.81 MiB/s, done.\n",
            "Resolving deltas: 100% (336/336), done.\n",
            "Checking out files: 100% (10020/10020), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjYJWpDrn7jQ"
      },
      "source": [
        "**required imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-MWsoiWOC8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18ff9c5-b33e-4817-a68c-1af52dd8a774"
      },
      "source": [
        "# Importing the required packages\n",
        "\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras import optimizers\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.recurrent import LSTM"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RGtsu1_n_kF"
      },
      "source": [
        "**building embedding for the words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7M9JpeG0CYa"
      },
      "source": [
        "#Define file paths required for the model\n",
        "\n",
        "# embedding bin file\n",
        "embed_file = \"/content/sim.expand.200d.vec\"\n",
        "\n",
        "#Define Hyper parameters\n",
        "max_inp_len = 20000\n",
        "# the dimension of vectors to be used\n",
        "embed_dim = 200\n",
        "rounding = 6\n",
        "# filter sizes of the different conv layers \n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 1\n",
        "pool_size = 199\n",
        "# dropout probability\n",
        "drop = 0.5\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "epochs = 30"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DbBN4tosBE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee7cb73-a5ed-4aa9-e2d8-603c1c948657"
      },
      "source": [
        "#define embedding dictionary and embed matrix for the vocabulary\n",
        "embeddings_dic = dict()\n",
        "f = open(embed_file,encoding='utf8')\n",
        "with open(embed_file, 'r', encoding='utf-8') as e_file:\n",
        "  for line in e_file:\n",
        "    splitlines = line.split()\n",
        "    word = splitlines[0].strip()\n",
        "    coefs = np.asarray(splitlines[1:], dtype='float32')\n",
        "    embeddings_dic[word] = coefs\n",
        "\n",
        "print(\"length of embedding dictionary\",len(embeddings_dic))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of embedding dictionary 70429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqa1tA-lt6e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2201a7ef-af8b-47f1-9fe2-c2a021da1400"
      },
      "source": [
        "vocabulary_size = len(embeddings_dic.keys())\n",
        "embed_token = Tokenizer()\n",
        "embed_token.fit_on_texts(embeddings_dic.keys())\n",
        "embedding_matrix = np.zeros((vocabulary_size, embed_dim))\n",
        "for word, index in embed_token.word_index.items():\n",
        "  embedding_matrix[index] = embeddings_dic.get(word)\n",
        "print(\"embedding_matrix dimension\",len(embedding_matrix),len(embedding_matrix[0]))\n",
        "print(\"no of token in the tokenizer\",len(embed_token.word_index) + 1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding_matrix dimension 70429 200\n",
            "no of token in the tokenizer 70429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rm4k5CZoOau"
      },
      "source": [
        "**Pre processing input and output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laF6m-NotXFw"
      },
      "source": [
        "#function to pre process the document\n",
        "def process_doc(path_file,embed_token) :\n",
        "\n",
        "  #tokenizing the words \n",
        "  with open(path_file,'r', encoding='utf-8') as tok_file :\n",
        "    file_words = list(tok_file)[0].split()\n",
        "    \n",
        "  #removing the stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_words = []  \n",
        "  for word in file_words: \n",
        "      if word not in stop_words and word.isalpha(): \n",
        "          filtered_words.append(word)\n",
        "\n",
        "  # applying stemming using PorterStemmer\n",
        "\n",
        "  p_stemmer = PorterStemmer()\n",
        "  stem_words=[]\n",
        "  for word in filtered_words:\n",
        "    stem_words.append(p_stemmer.stem(word))\n",
        "    \n",
        "  #tokenizing the words using the embed token\n",
        "  tokens=[]\n",
        "  for word in stem_words:\n",
        "    try:\n",
        "      tokens.append(embed_token.word_index[word])\n",
        "    except:\n",
        "      tokens.append(1)\n",
        "\n",
        "  if len(tokens) < max_inp_len:\n",
        "    tokens.extend([0]*(max_inp_len-len(tokens)))\n",
        "  else:\n",
        "    tokens = tokens[:max_inp_len]\n",
        "    \n",
        "  return np.array(tokens)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NCdDHfYt3l7"
      },
      "source": [
        "#output dataset\n",
        "def output_data(company_id, out_path_file):\n",
        "  with open(out_path_file,'r', encoding='utf-8') as out_file :\n",
        "    for line in out_file.readlines():\n",
        "      if company_id == line.split()[1]:\n",
        "        return line.split()[0]\n",
        "  return None"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BctWiw3UixYv"
      },
      "source": [
        "def pre_processing(meta_file,output_file):\n",
        "\n",
        "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
        "    \n",
        "    year = meta_file.split('/')[1].split('.')[0]\n",
        "    dir_path = os.path.dirname(meta_file) + '/' +year+'.tok'\n",
        "    data =[]\n",
        "\n",
        "    for line in m_file.readlines():\n",
        "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
        "      \n",
        "      # get input tokens from the company document\n",
        "      inp_tokens = process_doc(inp_path_file,embed_token)\n",
        "      \n",
        "      # get output value for the company\n",
        "      out_values = output_data(line.split()[0],output_file)\n",
        "\n",
        "      #insert values into the data list\n",
        "      data.append({'token':inp_tokens,'value':out_values})\n",
        "\n",
        "  return data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVi-HHNCPHkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9181a88-e660-4196-f983-0ab4a875feb3"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  ######## extracting text and storing it in dataframes ########\n",
        "  data_train = pre_processing('10k-sample/2007.meta.txt','10k-sample/2007.logvol.+12.txt')\n",
        "  data_train.extend(pre_processing('10k-sample/2008.meta.txt','10k-sample/2008.logvol.+12.txt'))\n",
        "  data_train.extend(pre_processing('10k-sample/2009.meta.txt','10k-sample/2009.logvol.+12.txt'))\n",
        "  train_df = pd.DataFrame(data_train,columns=['token','value'])\n",
        "  print(\"Length of training data\",len(data_train))\n",
        "\n",
        "\n",
        "  data_test = pre_processing('10k-sample/2010.meta.txt','10k-sample/2010.logvol.+12.txt')\n",
        "  test_df = pd.DataFrame(data_test,columns=['token','value'])\n",
        "  print(\"Length of testing data\",len(data_test))\n",
        "\n",
        "  print(\"SAMPLE INPUT TEXT AND VOLATILITY VALUES\")\n",
        "  print(train_df.sample(5)[['token','value']])\n",
        "  print(test_df.sample(5)[['token','value']])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training data 7571\n",
            "Length of testing data 2439\n",
            "SAMPLE INPUT TEXT AND VOLATILITY VALUES\n",
            "                                                  token     value\n",
            "5862  [170, 79, 197, 303, 45, 124, 25, 18, 69, 181, ...  -3.15361\n",
            "891   [170, 79, 197, 303, 45, 124, 25, 18, 837, 929,...  -3.96668\n",
            "3571  [170, 79, 197, 303, 45, 124, 25, 18, 150, 197,...  -3.34652\n",
            "4078  [170, 79, 197, 303, 45, 124, 25, 18, 254, 80, ...  -2.91691\n",
            "4567  [170, 79, 197, 303, 45, 124, 25, 18, 1007, 150...  -3.34225\n",
            "                                                  token     value\n",
            "896   [170, 79, 197, 303, 45, 124, 25, 18, 150, 197,...  -3.20688\n",
            "579   [170, 79, 197, 303, 45, 124, 25, 18, 26, 44, 1...  -3.46026\n",
            "586   [170, 79, 197, 303, 45, 124, 25, 18, 964, 6754...  -3.59318\n",
            "2182  [170, 79, 197, 303, 45, 124, 25, 18, 999, 121,...  -3.96111\n",
            "1081  [170, 79, 197, 303, 45, 124, 25, 18, 181, 78, ...   -4.1661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fevays6tkt2p"
      },
      "source": [
        "RNN_train_input = train_df.token.values\n",
        "RNN_train_output = [ float(x) for x in train_df.value.values ]\n",
        "RNN_test_input = test_df.token.values\n",
        "RNN_test_output = [ float(x) for x in test_df.value.values ]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG6qwiQ-kffG"
      },
      "source": [
        "RNN_train_output = np.array(RNN_train_output).reshape(len(RNN_train_output),1)\n",
        "RNN_test_output = np.array(RNN_test_output).reshape(len(RNN_test_output),1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcgQtUmJooLV"
      },
      "source": [
        "**define the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIOmvEheA_Mw"
      },
      "source": [
        "def define_model(max_inp_len,vocabulary_size,embed_dim,filter_sizes,num_filters,pool_size,drop,learning_rate):\n",
        "  \n",
        "  # input and embedding matrix\n",
        "  inputs = Input(shape=(max_inp_len,))\n",
        "  embedding = Embedding(vocabulary_size, embed_dim, weights=[embedding_matrix],trainable = False)(inputs)\n",
        "\n",
        "  custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
        "\n",
        " # Layer1 RNN with LSTM\n",
        "  layer_1 = LSTM(units=64, activation = 'tanh' )(embedding)\n",
        "       \n",
        "  # 1 fully connected layers\n",
        "  outputs = Dense(1, activation=custom_objects['leaky_relu'])(layer_1)\n",
        "\n",
        "  model = Model(inputs=[inputs], outputs=outputs)\n",
        "    \n",
        "  opt = optimizers.SGD(learning_rate=learning_rate)\n",
        "  model.compile(loss='mse', optimizer=opt)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3xv22helsjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf923e7-1bc4-45bb-9419-10ee6f24f3cd"
      },
      "source": [
        "# define model\n",
        "model = define_model(max_inp_len,vocabulary_size,embed_dim,filter_sizes,num_filters,pool_size,drop,learning_rate)\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 20000)]           0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 20000, 200)        14085800  \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                67840     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 14,153,705\n",
            "Trainable params: 67,905\n",
            "Non-trainable params: 14,085,800\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "288-rkCSouut"
      },
      "source": [
        "**Fit the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Eah3K9ombxX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea51cc3f-b480-409f-a4ad-d71c1d2d4ff9"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "n_splits = 3\n",
        "epochs = 8\n",
        "RNN_train_input = np.stack(RNN_train_input)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  kf = KFold(n_splits=n_splits)\n",
        "  history =[]\n",
        "  train_loss=[]\n",
        "  vald_loss=[]\n",
        "  fold = 1\n",
        "  for train_index, test_index in kf.split(RNN_train_input):\n",
        "    \n",
        "    checkpoint_filepath = 'RNN_results/CheckPoints/RNN_checkpoint'+str(fold)\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "    train_history = model.fit(\n",
        "                              RNN_train_input[train_index],\n",
        "                              RNN_train_output[train_index],#output\n",
        "                              epochs=epochs, #epochs\n",
        "                              verbose=1,\n",
        "                              callbacks=[model_checkpoint_callback]\n",
        "                          )\n",
        "    model_best = tf.keras.models.load_model(checkpoint_filepath)\n",
        "    fold+=1\n",
        "    loss_T = model_best.evaluate(RNN_train_input[train_index],RNN_train_output[train_index], verbose=0)\n",
        "    loss_V = model_best.evaluate(RNN_train_input[test_index],RNN_train_output[test_index], verbose=0)\n",
        "    print(loss_T,loss_V)\n",
        "    train_loss.append(loss_T)\n",
        "    vald_loss.append(loss_V)\n",
        "    history.append(train_history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "158/158 [==============================] - 144s 718ms/step - loss: 9.8881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/8\n",
            "158/158 [==============================] - 114s 719ms/step - loss: 9.2093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/8\n",
            "158/158 [==============================] - 115s 726ms/step - loss: 8.0550\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/8\n",
            "158/158 [==============================] - 116s 732ms/step - loss: 5.7862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/8\n",
            "158/158 [==============================] - 116s 734ms/step - loss: 3.4404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/8\n",
            "158/158 [==============================] - 116s 734ms/step - loss: 1.9105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/8\n",
            "158/158 [==============================] - 116s 736ms/step - loss: 0.9997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/8\n",
            "158/158 [==============================] - 117s 738ms/step - loss: 0.5491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: RNN_results/CheckPoints/RNN_checkpoint1/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.38153186440467834 0.7377358078956604\n",
            "Epoch 1/8\n",
            "114/158 [====================>.........] - ETA: 32s - loss: 0.5077"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOnbGUpWo0R-"
      },
      "source": [
        "**Plotting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWotBg1FnINm"
      },
      "source": [
        "plt.plot(train_loss, label = \"Trainng Loss\")\n",
        "plt.plot(vald_loss, label = \"Validation Loss\")\n",
        "# naming the x axis \n",
        "plt.xlabel('Folds') \n",
        "# naming the y axis \n",
        "plt.ylabel('Error') \n",
        "# function to show the plot \n",
        "plt.legend()\n",
        "plt.savefig('RNN_results/Plots/RNN_loss_check.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LtLP387nlI0"
      },
      "source": [
        "test_loss = []\n",
        "RNN_test_input = np.stack(RNN_test_input)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  for i in range(n_splits):\n",
        "    checkpoint_filepath = 'RNN_results/CheckPoints/RNN_checkpoint'+str(i+1) \n",
        "    best_model = tf.keras.models.load_model(\n",
        "                                            checkpoint_filepath, custom_objects=None, compile=True, options=None\n",
        "                                            )\n",
        "    predicted = best_model.predict(RNN_test_input)\n",
        "    \n",
        "    loss_test = best_model.evaluate(RNN_test_input,RNN_test_output, verbose=0)\n",
        "    print(\"Test Errror for the fold \",i+1,\" is\",loss_test )\n",
        "    \n",
        "    \n",
        "    plt.plot(predicted[0:50], label = \"Predicted Values\")  \n",
        "    plt.plot(RNN_test_output[0:50], label = \"Actual Values\")\n",
        "    # naming the x axis \n",
        "    plt.xlabel('Test Samples') \n",
        "    # naming the y axis \n",
        "    plt.ylabel('Output Values') \n",
        "    # function to show the plot \n",
        "    plt.legend()\n",
        "    textstr = \"Test Errror for the fold \"+ str(i+1)+\" is \"+str(np.round(loss_test,3))\n",
        "    plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
        "    plt.savefig('RNN_results/Plots/RNN_fold'+str(i+1)+'.png',bbox_inches='tight')\n",
        "    plt.clf()\n",
        "\n",
        "    test_loss.append(loss_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxzqg1mj3Opv"
      },
      "source": [
        "!git remote add origin https://sravanisasubilli.ss:sravani$19MCMB14@github.com/sravanisasu/BERT_Regression.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j2fD9GK1Wni"
      },
      "source": [
        "uname = \"sravanisasubilli.ss\"\n",
        "!git config --global user.email '$uname@gmail.com'\n",
        "!git config --global user.name '$uname'\n",
        "\n",
        "\n",
        "password = 'sravani$19MCMB14'\n",
        "!git clone https://$uname:$password@github.com/sravanisasu/BERT_Regression \n",
        "\n",
        "         # push to github"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q2MKmMZ2UE3"
      },
      "source": [
        "# create a file, then add it to stage\n",
        "!git add /content/RNN_results/Plots/RNN_fold1.png\n",
        "!git commit -m 'commit message'  # commit in Colab\n",
        "!git push origin master "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBxQA0iUnsfB"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "data=[]\n",
        "data.append(train_loss)\n",
        "data.append(vald_loss)\n",
        "data.append(test_loss)\n",
        "  \n",
        "fig = plt.figure()  \n",
        "# Creating axes instance \n",
        "ax = fig.add_axes([0, 0, 1, 1]) \n",
        "  \n",
        "# Creating plot \n",
        "ax.boxplot(data)\n",
        "\n",
        "ax.set_xticklabels(['Training', 'Validation','Test']) \n",
        "\n",
        "# naming the y axis \n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title(\"Box plot for Training, Validation and Test Loss\")\n",
        "textstr ='Training Loss  : '+str(np.round(np.mean(train_loss),3))+' ('+str(np.round(np.std(train_loss),3))+')\\n'+'Validation Loss  : '+str(np.round(np.mean(vald_loss),3))+' ('+str(np.round(np.std(vald_loss),3))+')\\n'+'Test Loss  : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
        "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
        "# show plot \n",
        "plt.savefig('RNN_results/Plots/block_RNN.png',bbox_inches='tight')\n",
        "\n",
        "print('Training Loss: %.3f (%.3f)' % (np.mean(train_loss), np.std(train_loss)))\n",
        "print('Validation Loss: %.3f (%.3f)' % (np.mean(vald_loss), np.std(vald_loss)))\n",
        "print('Test Loss: %.3f (%.3f)' % (np.mean(test_loss), np.std(test_loss)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}