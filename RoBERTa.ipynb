{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Necessary imports and installations for the implementation of RoBERTa Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import os as os\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.metrics import MeanSquaredError\n",
    "from transformers import RobertaConfig\n",
    "from transformers import TFRobertaModel\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a RoBERTa layer from the tensorflow-hub library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained('roberta-base')\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-base',config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to preprocess input 10-K documents and output values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to extract the input text from the files ########\n",
    "def process_inp_doc(path_file) :\n",
    "\n",
    "  file_text = open(path_file,encoding='utf8').read()\n",
    "\n",
    "  # remove punctations and digits and remove <PAGE> which was used for page number\n",
    "  file_data = re.sub(r'[\\d$%-:;!]', '', file_text)\n",
    "  file_data = re.sub(r'<PAGE>', '', file_data)\n",
    "  file_data = ''.join(file_data)\n",
    "\n",
    "  return file_data\n",
    "\n",
    "######## Function to extract the output values from the file ########\n",
    "def process_out(company_id,output_file):\n",
    "  \n",
    "  with open(output_file,'r', encoding='utf-8') as m_file :\n",
    "    for line in m_file.readlines():\n",
    "      if company_id == line.split()[1]:\n",
    "        return line.split()[0]\n",
    "    print(\"not found\")\n",
    "  return None\n",
    "\n",
    "######## Function to pre-process the documents from meta-file of a given year ########\n",
    "def pre_processing(meta_file,output_file):\n",
    "  \n",
    "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
    "    \n",
    "    year = meta_file.split('/')[1].split('.')[0]\n",
    "    dir_path = os.path.dirname(meta_file) + '/' +year+'.tok'\n",
    "    data =[]\n",
    "    for line in m_file.readlines():\n",
    "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
    "\n",
    "      # get input sentences from the company document\n",
    "      inp_sentences = process_inp_doc(inp_path_file)\n",
    "    \n",
    "      # get output value for the company\n",
    "      out_values = float(process_out(line.split()[0],output_file))\n",
    "\n",
    "      #insert values into the data list\n",
    "      data.append({'text':inp_sentences,'value':out_values})\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to get the embeddings(token,masked,segment) and to encode the text for the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to get the encoded values ######## \n",
    "def roberta_encode(sentences, tokenizer, MAX_SEQ_LEN=512):\n",
    "\n",
    "  all_tokens = []\n",
    "  all_masks = []\n",
    "  all_segments = []\n",
    "  for sentence in sentences:\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = stokens[-MAX_SEQ_LEN+2:]\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(stokens,)\n",
    "\n",
    "    ids = token_ids + [0] * (MAX_SEQ_LEN-len(token_ids))\n",
    "    masks = [1]*len(token_ids) + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    segments = [0] * (MAX_SEQ_LEN)\n",
    "\n",
    "    all_tokens.append(ids)\n",
    "    all_masks.append(masks)\n",
    "    all_segments.append(segments)\n",
    "\n",
    "  return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data 7571\n",
      "Length of testing data 2439\n",
      "SAMPLE INPUT TEXT AND VOLATILITY VALUES\n",
      "                                                   text    value\n",
      "5047  item # management s discussion and analysis of... -2.60301\n",
      "3371  item # management s discussion and analysis of... -2.96628\n",
      "1912  item # management s discussion and analysis of... -3.12003\n",
      "7137  item # management s discussion and analysis of... -3.61965\n",
      "6688  item # management s discussion and analysis of... -3.23547\n",
      "                                                   text    value\n",
      "688   item # management s discussion and analysis of... -3.38065\n",
      "361   item # management s discussion and analysis of... -4.14227\n",
      "96    item # management s discussion and analysis of... -4.25670\n",
      "1384  item # management s discussion and analysis of... -3.41319\n",
      "1054  item # management s discussion and analysis of... -3.39300\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "  ######## extracting text and storing it in dataframes ########\n",
    "  data_train = pre_processing('10k-sample/2007.meta.txt','10k-sample/2007.logvol.+12.txt')\n",
    "  data_train.extend(pre_processing('10k-sample/2008.meta.txt','10k-sample/2008.logvol.+12.txt'))\n",
    "  data_train.extend(pre_processing('10k-sample/2009.meta.txt','10k-sample/2009.logvol.+12.txt'))\n",
    "  train_df = pd.DataFrame(data_train,columns=['text','value'])\n",
    "  print(\"Length of training data\",len(data_train))\n",
    "\n",
    "  data_test = pre_processing('10k-sample/2010.meta.txt','10k-sample/2010.logvol.+12.txt')\n",
    "  test_df = pd.DataFrame(data_test,columns=['text','value'])\n",
    "  print(\"Length of testing data\",len(data_test))\n",
    "\n",
    "  print(\"SAMPLE INPUT TEXT AND VOLATILITY VALUES\")\n",
    "  print(train_df.sample(5)[['text','value']])\n",
    "  print(test_df.sample(5)[['text','value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text    value\n",
      "0     item # management s discussion and analysis of... -3.46398\n",
      "1     item # management s discussion and analysis of... -3.58048\n",
      "2     item # management s discussion and analysis of... -3.87840\n",
      "3     item # management s discussion and analysis of... -3.37969\n",
      "4     item # management s discussion and analysis of... -4.34506\n",
      "...                                                 ...      ...\n",
      "7566  item # management s discussion and analysis of... -2.75096\n",
      "7567  item # management s discussion and analysis of... -3.46372\n",
      "7568  item # management s discussion and analysis of... -2.94439\n",
      "7569  item # management s discussion and analysis of... -3.27556\n",
      "7570  item # management s discussion and analysis of... -3.33055\n",
      "\n",
      "[6717 rows x 2 columns]\n",
      "                                                   text    value\n",
      "0     item # management s discussion and analysis of... -3.87816\n",
      "1     item # management s discussion and analysis of... -3.45482\n",
      "2     item # management s discussion and analysis of... -3.78896\n",
      "3     item # management s discussion and analysis of... -3.85686\n",
      "4     item # management s discussion and analysis of... -4.71308\n",
      "...                                                 ...      ...\n",
      "2434  item # management s discussion and analysis of... -3.59450\n",
      "2435  item # management s discussion and analysis of... -3.60882\n",
      "2436  item # management s discussion and analysis of... -3.74614\n",
      "2437  item # management s discussion and analysis of... -3.51231\n",
      "2438  item # management s discussion and analysis of... -3.40038\n",
      "\n",
      "[2178 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.loc[train_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "print(train_df)\n",
    "#88.7%\n",
    "test_df = test_df.loc[test_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "print(test_df)\n",
    "#89.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a446ef0a20c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m######## extracting tokens from dataframes ########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mMAX_SEQ_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "######## extracting tokens from dataframes ########\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  \n",
    "  #### training \n",
    "  # input encoding\n",
    "  sentences = train_df.text.values\n",
    "  roberta_train_input = roberta_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "  # output values\n",
    "  roberta_train_output = train_df.value.values\n",
    "\n",
    "  #### test\n",
    "  # input encoding\n",
    "  sentences = test_df.text.values\n",
    "  roberta_test_input = roberta_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "  # output values\n",
    "  roberta_test_output = test_df.value.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "roberta_train_output = np.array(roberta_train_output).reshape(len(roberta_train_output),1)\n",
    "roberta_test_output = np.array(roberta_test_output).reshape(len(roberta_test_output),1)\n",
    "output = np.concatenate((roberta_train_output, roberta_test_output))\n",
    "output = scaler.fit_transform(output)\n",
    "roberta_train_output = output[:len(roberta_train_input[0])]\n",
    "roberta_test_output = output[-len(roberta_test_input[0]):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function that define the model architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "  input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_word_ids\")\n",
    "  input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_mask\")\n",
    "  segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"segment_ids\")\n",
    "\n",
    "  custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
    "\n",
    "  pooled_output, sequence_output = roberta_model([input_word_ids, input_mask, segment_ids])\n",
    "  clf_output = pooled_output\n",
    "  \n",
    "  net = tf.keras.layers.Dense(64, activation=custom_objects['leaky_relu'])(clf_output)\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = Flatten()(net)\n",
    "  out = tf.keras.layers.Dense(1, activation=custom_objects['leaky_relu'])(net)\n",
    "\n",
    "  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "\n",
    "  opt = optimizers.Adam(learning_rate=0.05)\n",
    "  model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "epochs = 50\n",
    "with tf.device('/device:GPU:0'):\n",
    "  kf = KFold(n_splits=n_splits)\n",
    "  history =[]\n",
    "  train_loss=[]\n",
    "  vald_loss=[]\n",
    "  fold = 1\n",
    "  for train_index, test_index in kf.split(roberta_train_input[0]):\n",
    "    \n",
    "    checkpoint_filepath = 'RoBERTa_results/CheckPoints/roberta_checkpoint'+str(fold)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "    train_history = model.fit(\n",
    "                              [roberta_train_input[0][train_index],roberta_train_input[1][train_index],roberta_train_input[2][train_index]],#input\n",
    "                              roberta_train_output[train_index],#output\n",
    "                              epochs=epochs, #epochs\n",
    "                              verbose=1,\n",
    "                              callbacks=[model_checkpoint_callback]\n",
    "                          )\n",
    "    model_best = tf.keras.models.load_model(checkpoint_filepath)\n",
    "    fold+=1\n",
    "    loss_T = model_best.evaluate([roberta_train_input[0][train_index],roberta_train_input[1][train_index],roberta_train_input[2][train_index]]\n",
    "                                       , roberta_train_output[train_index], verbose=0)\n",
    "    loss_V = model_best.evaluate([roberta_train_input[0][test_index],roberta_train_input[1][test_index],roberta_train_input[2][test_index]]\n",
    "                                      , roberta_train_output[test_index], verbose=0)\n",
    "    print(loss_T,loss_V)\n",
    "    train_loss.append(loss_T)\n",
    "    vald_loss.append(loss_V)\n",
    "    history.append(train_history)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot all the folds together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label = \"Trainng Loss\")\n",
    "plt.plot(vald_loss, label = \"Validation Loss\")\n",
    "# naming the x axis \n",
    "plt.xlabel('Folds') \n",
    "# naming the y axis \n",
    "plt.ylabel('Error') \n",
    "# function to show the plot \n",
    "plt.legend()\n",
    "plt.savefig('RoBERTa_results/Plots/roberta_loss_check.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict the Model from the checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = []\n",
    "with tf.device('/device:GPU:0'):\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "\n",
    "        checkpoint_filepath = 'RoBERTa_results/CheckPoints/roberta_checkpoint'+str(i+1) \n",
    "        best_model = tf.keras.models.load_model(\n",
    "                                                checkpoint_filepath, custom_objects=None, compile=True, options=None\n",
    "                                                )\n",
    "        predicted = best_model.predict(roberta_test_input[0:50])\n",
    "        \n",
    "        loss_test = best_model.evaluate([roberta_test_input[0],roberta_test_input[1],roberta_test_input[2]]\n",
    "                                          , roberta_test_output, verbose=0)\n",
    "        print(\"Test Errror for the fold \",i+1,\" is\",loss_test )\n",
    "        \n",
    "        \n",
    "        plt.plot(predicted[0:50], label = \"Predicted Values\")  \n",
    "        plt.plot(roberta_test_output[0:50], label = \"Actual Values\")\n",
    "        # naming the x axis \n",
    "        plt.xlabel('Test Samples') \n",
    "        # naming the y axis \n",
    "        plt.ylabel('Output Values') \n",
    "        # function to show the plot \n",
    "        plt.legend()\n",
    "        textstr = \"Test Errror for the fold \"+ str(i+1)+\" is \"+str(np.round(loss_test,3))\n",
    "        plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
    "        plt.savefig('RoBERTa_results/Plots/roberta_fold'+str(i+1)+'.png',bbox_inches='tight')\n",
    "        plt.clf()\n",
    "\n",
    "        test_loss.append(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "data=[]\n",
    "data.append(train_loss)\n",
    "data.append(vald_loss)\n",
    "data.append(test_loss)\n",
    "  \n",
    "fig = plt.figure()  \n",
    "# Creating axes instance \n",
    "ax = fig.add_axes([0, 0, 1, 1]) \n",
    "  \n",
    "# Creating plot \n",
    "ax.boxplot(data)\n",
    "\n",
    "ax.set_xticklabels(['Training', 'Validation','Test']) \n",
    "\n",
    "# naming the y axis \n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title(\"Box plot for Training, Validation and Test Loss\")\n",
    "textstr ='Training Loss  : '+str(np.round(np.mean(train_loss),3))+' ('+str(np.round(np.std(train_loss),3))+')\\n'+'Validation Loss  : '+str(np.round(np.mean(vald_loss),3))+' ('+str(np.round(np.std(vald_loss),3))+')\\n'+'Test Loss  : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
    "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
    "# show plot \n",
    "plt.savefig('RoBERTa_results/Plots/block_roberta.png',bbox_inches='tight')\n",
    "\n",
    "print('Training Loss: %.3f (%.3f)' % (np.mean(train_loss), np.std(train_loss)))\n",
    "print('Validation Loss: %.3f (%.3f)' % (np.mean(vald_loss), np.std(vald_loss)))\n",
    "print('Test Loss: %.3f (%.3f)' % (np.mean(test_loss), np.std(test_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
