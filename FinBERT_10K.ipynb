{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinBERT-10K.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2PvMVHYsP3zreyzKNlwWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravanisasu/BERT_Regression/blob/main/FinBERT_10K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VbYRib15Wad"
      },
      "source": [
        "**Setup GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6zYGrcRv_Z4",
        "outputId": "bc05a67f-e6c8-44ab-9629-fb181068df56"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFN5c5QkUZvQ"
      },
      "source": [
        "**Clone data from github**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDbcSqGrYEfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca616232-e8a2-4df4-97b1-371f60fa57d8"
      },
      "source": [
        "!git clone https://github.com/sravanisasu/10k-sample"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '10k-sample'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 9557 (delta 1), reused 1 (delta 0), pack-reused 9548\u001b[K\n",
            "Receiving objects: 100% (9557/9557), 158.15 MiB | 21.26 MiB/s, done.\n",
            "Resolving deltas: 100% (336/336), done.\n",
            "Checking out files: 100% (10020/10020), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVmuV__8vMHR"
      },
      "source": [
        "**Necessary imports and installations for the implementation of FinBERT Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkV9qOoAZAep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43701ee2-9638-47a9-b9ac-8ce0747ac89b"
      },
      "source": [
        "% pip install sentencepiece\n",
        "% pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 21.2MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 27.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 31.1MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 33.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 29.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 25.5MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 21.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 22.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 20.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 19.1MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 19.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 19.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 19.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 19.1MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 19.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 19.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 19.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 15.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.5MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 57.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=cf01ca7ecb39bcc1553121a9e83ec4b9167a059529015f54b4ff7546a8b50b7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef0WywVmvTsZ"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import os as os\n",
        "import regex as re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer,BertConfig\n",
        "from transformers import TFBertModel\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.metrics import MeanSquaredError\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhdxl-IeeuhK"
      },
      "source": [
        "**Create a FinBERT model from the transformers library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39rNkQ6fecnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60ff324-e04b-4f3e-bbdd-14dd5613701e"
      },
      "source": [
        "config = BertConfig.from_pretrained(vocab_path='/content/FinVocab-Uncased.txt',pretrained_model_name_or_path='/content/config.json')\n",
        "FinBERT_model = TFBertModel.from_pretrained(config=config,pretrained_model_name_or_path='/content/drive/MyDrive/Colab Notebooks/pytorch_model.bin',from_pt=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktCJhz_Il9vt",
        "outputId": "02e42601-e344-44c5-9fb9-74ba3ce9b428"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMpQFX7bXO1X",
        "outputId": "0ba65622-ce6e-465e-805d-c9689efb80e5"
      },
      "source": [
        "!git clone https://github.com/sravanisasu/analyst_tone/blob/main/config.json"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'config.json' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdZmjVaXUe3Q"
      },
      "source": [
        "**Functions to preprocess input 10-K documents and output values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKJ0Aih2TEIk"
      },
      "source": [
        "######## Function to extract the input text from the files ########\n",
        "def process_inp_doc(path_file) :\n",
        "\n",
        "  file_text = open(path_file,encoding='utf8').read()\n",
        "\n",
        "  # remove punctations and digits and remove <PAGE> which was used for page number\n",
        "  file_data = re.sub(r'[\\d$%-:;!]', '', file_text)\n",
        "  file_data = re.sub(r'<PAGE>', '', file_data)\n",
        "  file_data = ''.join(file_data)\n",
        "\n",
        "  return file_data\n",
        "\n",
        "######## Function to extract the output values from the file ########\n",
        "def process_out(company_id,output_file):\n",
        "  \n",
        "  with open(output_file,'r', encoding='utf-8') as m_file :\n",
        "    for line in m_file.readlines():\n",
        "      if company_id == line.split()[1]:\n",
        "        return line.split()[0]\n",
        "    print(\"not found\")\n",
        "  return None\n",
        "\n",
        "######## Function to pre-process the documents from meta-file of a given year ########\n",
        "def pre_processing(meta_file,output_file):\n",
        "  \n",
        "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
        "    \n",
        "    year = meta_file.split('/')[3].split('.')[0]\n",
        "    dir_path = os.path.dirname(meta_file) + '/' +year+'.tok'\n",
        "    data =[]\n",
        "    \n",
        "    for line in m_file.readlines():\n",
        "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
        "\n",
        "      # get input sentences from the company document\n",
        "      inp_sentences = process_inp_doc(inp_path_file)\n",
        "    \n",
        "      # get output value for the company\n",
        "      out_values = float(process_out(line.split()[0],output_file))\n",
        "\n",
        "      #insert values into the data list\n",
        "      data.append({'text':inp_sentences,'value':out_values})\n",
        "\n",
        "  return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AImjMKRcqhrT"
      },
      "source": [
        "**Functions to get the embeddings(token,masked,segment) and to encode the text for the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UqDE8B9e1H8"
      },
      "source": [
        "######## Function to get the encoded values ######## \n",
        "def FinBERT_encode(sentences, tokenizer, MAX_SEQ_LEN=512):\n",
        "\n",
        "  all_tokens = []\n",
        "  all_masks = []\n",
        "  all_segments = []\n",
        "  for sentence in sentences:\n",
        "    stokens = tokenizer.tokenize(sentence)\n",
        "    stokens = stokens[-MAX_SEQ_LEN+2:]\n",
        "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(stokens,)\n",
        "\n",
        "    ids = token_ids + [0] * (MAX_SEQ_LEN-len(token_ids))\n",
        "    masks = [1]*len(token_ids) + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
        "    segments = [0] * (MAX_SEQ_LEN)\n",
        "\n",
        "    all_tokens.append(ids)\n",
        "    all_masks.append(masks)\n",
        "    all_segments.append(segments)\n",
        "\n",
        "  return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc2nh_RyvagZ"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miBW5NQ7vhwh",
        "outputId": "71052621-27be-4f9c-84d2-37d8fe62e3bf"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  ######## extracting text and storing it in dataframes ########\n",
        "  data_train = pre_processing('/content/10k-sample/2007.meta.txt','/content/10k-sample/2007.logvol.+12.txt')\n",
        "  data_train.extend(pre_processing('/content/10k-sample/2008.meta.txt','/content/10k-sample/2008.logvol.+12.txt'))\n",
        "  data_train.extend(pre_processing('/content/10k-sample/2009.meta.txt','/content/10k-sample/2009.logvol.+12.txt'))\n",
        "  train_df = pd.DataFrame(data_train,columns=['text','value'])\n",
        "  print(\"Length of training data\",len(data_train))\n",
        "\n",
        "  data_test = pre_processing('/content/10k-sample/2010.meta.txt','/content/10k-sample/2010.logvol.+12.txt')\n",
        "  test_df = pd.DataFrame(data_test,columns=['text','value'])\n",
        "  print(\"Length of testing data\",len(data_test))\n",
        "\n",
        "  print(\"SAMPLE INPUT TEXT AND VOLATILITY VALUES\")\n",
        "  print(train_df.sample(5)[['text','value']])\n",
        "  print(test_df.sample(5)[['text','value']])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training data 7571\n",
            "Length of testing data 2439\n",
            "SAMPLE INPUT TEXT AND VOLATILITY VALUES\n",
            "                                                   text    value\n",
            "5077  item # management s discussion and analysis of... -4.15766\n",
            "2510  item # management s discussion and analysis of... -2.70976\n",
            "513   item # management s discussion and analysis of... -4.05281\n",
            "3267  item # management s discussion and analysis of... -3.09347\n",
            "196   item # management s discussion and analysis of... -2.64426\n",
            "                                                   text    value\n",
            "138   item # management s discussion and analysis of... -4.00342\n",
            "2438  item # management s discussion and analysis of... -3.40038\n",
            "1248  item # management s discussion and analysis of... -3.66267\n",
            "913   item # management s discussion and analysis of... -2.79292\n",
            "198   item # management s discussion and analysis of... -3.38210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTdv6Jj7fhmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c80658-0acf-4217-e8b9-108b806af595"
      },
      "source": [
        "train_df = train_df.loc[train_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
        "print(train_df)\n",
        "#88.7%\n",
        "test_df = test_df.loc[test_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
        "print(test_df)\n",
        "#89.3%"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   text    value\n",
            "0     item # management s discussion and analysis of... -3.46398\n",
            "1     item # management s discussion and analysis of... -3.58048\n",
            "2     item # management s discussion and analysis of... -3.87840\n",
            "3     item # management s discussion and analysis of... -3.37969\n",
            "4     item # management s discussion and analysis of... -4.34506\n",
            "...                                                 ...      ...\n",
            "7566  item # management s discussion and analysis of... -2.75096\n",
            "7567  item # management s discussion and analysis of... -3.46372\n",
            "7568  item # management s discussion and analysis of... -2.94439\n",
            "7569  item # management s discussion and analysis of... -3.27556\n",
            "7570  item # management s discussion and analysis of... -3.33055\n",
            "\n",
            "[6717 rows x 2 columns]\n",
            "                                                   text    value\n",
            "0     item # management s discussion and analysis of... -3.87816\n",
            "1     item # management s discussion and analysis of... -3.45482\n",
            "2     item # management s discussion and analysis of... -3.78896\n",
            "3     item # management s discussion and analysis of... -3.85686\n",
            "4     item # management s discussion and analysis of... -4.71308\n",
            "...                                                 ...      ...\n",
            "2434  item # management s discussion and analysis of... -3.59450\n",
            "2435  item # management s discussion and analysis of... -3.60882\n",
            "2436  item # management s discussion and analysis of... -3.74614\n",
            "2437  item # management s discussion and analysis of... -3.51231\n",
            "2438  item # management s discussion and analysis of... -3.40038\n",
            "\n",
            "[2178 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqgmNzQ46Ya7"
      },
      "source": [
        "MAX_SEQ_LEN = 512\n",
        "\n",
        "vocab_path = '/content/FinVocab-Uncased.txt'\n",
        "######## extracting tokens from dataframes ########\n",
        "\n",
        "tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, do_basic_tokenize = True)\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  #### training \n",
        "  # input encoding\n",
        "  sentences = train_df.text.values\n",
        "  FinBERT_train_input = FinBERT_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
        "  # output values\n",
        "  FinBERT_train_output = train_df.value.values\n",
        "\n",
        "  #### test\n",
        "  # input encoding\n",
        "  sentences = test_df.text.values\n",
        "  FinBERT_test_input = FinBERT_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
        "  # output values\n",
        "  FinBERT_test_output = test_df.value.values"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJFRdYgYfux9"
      },
      "source": [
        "FinBERT_train_output = np.array(FinBERT_train_output).reshape(len(FinBERT_train_output),1)\n",
        "FinBERT_test_output = np.array(FinBERT_test_output).reshape(len(FinBERT_test_output),1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1milpeqreHi"
      },
      "source": [
        "**Function that define the model architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0uDtPqYirVK"
      },
      "source": [
        "def get_model():\n",
        "\n",
        "  input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_word_ids\")\n",
        "  input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_mask\")\n",
        "  segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"segment_ids\")\n",
        "\n",
        "  custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
        "\n",
        "  model_output = FinBERT_model([input_word_ids, input_mask, segment_ids])\n",
        "  clf_output = model_output.pooler_output\n",
        "  print(type(model_output.pooler_output))\n",
        "  \n",
        "  net = tf.keras.layers.Dense(64, activation=custom_objects['leaky_relu'])(clf_output)\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Flatten()(net)\n",
        "  out = tf.keras.layers.Dense(1, activation=custom_objects['leaky_relu'])(net)\n",
        "\n",
        "  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "\n",
        "  opt = optimizers.Adam(learning_rate=0.05)\n",
        "  model.compile(optimizer=opt, loss='mse')\n",
        "\n",
        "  return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1VL1F7vsjbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57210e6a-948a-4ee0-cb37-0499f2612cfd"
      },
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7ff68fa69ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7ff68fa69ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7ff6ab31cc20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7ff6ab31cc20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "<class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109751808   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 64)           49216       tf_bert_model[0][1]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 64)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 64)           0           dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            65          flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 109,801,089\n",
            "Trainable params: 109,801,089\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz3mOcj93ZrT"
      },
      "source": [
        "**Fit the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1ESuvq9eyrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6762a085-a6bf-4adc-96a7-c8f3aa6f67ab"
      },
      "source": [
        "n_splits = 2\n",
        "epochs = 15\n",
        "batch_size = 10\n",
        "with tf.device('/device:GPU:0'):\n",
        "  kf = KFold(n_splits=n_splits)\n",
        "  history =[]\n",
        "  train_loss=[]\n",
        "  vald_loss=[]\n",
        "  fold = 1\n",
        "  for train_index, test_index in kf.split(FinBERT_train_input[0]):\n",
        "    \n",
        "    checkpoint_filepath = 'FinBERT_results/CheckPoints/FinBERT_checkpoint'+str(fold)\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "    train_history = model.fit(\n",
        "                              [FinBERT_train_input[0][train_index],FinBERT_train_input[1][train_index],FinBERT_train_input[2][train_index]],#input\n",
        "                              FinBERT_train_output[train_index],#output\n",
        "                              epochs=epochs, #epochs\n",
        "                              verbose=1 ,\n",
        "                              batch_size = batch_size,\n",
        "                              callbacks=[model_checkpoint_callback]\n",
        "                          )\n",
        "    model_best = tf.keras.models.load_model(checkpoint_filepath)\n",
        "    fold+=1\n",
        "    loss_T = model_best.evaluate([FinBERT_train_input[0][train_index],FinBERT_train_input[1][train_index],FinBERT_train_input[2][train_index]]\n",
        "                                       , FinBERT_train_output[train_index], verbose=0)\n",
        "    loss_V = model_best.evaluate([FinBERT_train_input[0][test_index],FinBERT_train_input[1][test_index],FinBERT_train_input[2][test_index]]\n",
        "                                      , FinBERT_train_output[test_index], verbose=0)\n",
        "    print(loss_T,loss_V)\n",
        "    train_loss.append(loss_T)\n",
        "    vald_loss.append(loss_V)\n",
        "    history.append(train_history)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            " 59/336 [====>.........................] - ETA: 5:25 - loss: 613.4013"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU-U3Qu_WmAi"
      },
      "source": [
        "**Plot the results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i704HZcWrdH"
      },
      "source": [
        "plt.plot(train_loss, label = \"Trainng Loss\")\n",
        "plt.plot(vald_loss, label = \"Validation Loss\")\n",
        "# naming the x axis \n",
        "plt.xlabel('Folds') \n",
        "# naming the y axis \n",
        "plt.ylabel('Error') \n",
        "# function to show the plot \n",
        "plt.legend()\n",
        "plt.savefig('FinBERT_results/Plots/FinBERT_loss_check.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yiCNOvgWt2k"
      },
      "source": [
        "test_loss = []\n",
        "with tf.device('/device:GPU:0'):\n",
        "    \n",
        "    for i in range(n_splits):\n",
        "\n",
        "        checkpoint_filepath = 'FinBERT_results/CheckPoints/FinBERT_checkpoint'+str(i+1) \n",
        "        best_model = tf.keras.models.load_model(\n",
        "                                                checkpoint_filepath, custom_objects=None, compile=True, options=None\n",
        "                                                )\n",
        "        predicted = best_model.predict(FinBERT_test_input[0:50])\n",
        "        \n",
        "        loss_test = best_model.evaluate([FinBERT_test_input[0],FinBERT_test_input[1],FinBERT_test_input[2]]\n",
        "                                          , FinBERT_test_output, verbose=0)\n",
        "        print(\"Test Errror for the fold \",i+1,\" is\",loss_test )\n",
        "        \n",
        "        \n",
        "        plt.plot(predicted[0:50], label = \"Predicted Values\")  \n",
        "        plt.plot(FinBERT_test_output[0:50], label = \"Actual Values\")\n",
        "        # naming the x axis \n",
        "        plt.xlabel('Test Samples') \n",
        "        # naming the y axis \n",
        "        plt.ylabel('Output Values') \n",
        "        # function to show the plot \n",
        "        plt.legend()\n",
        "        textstr = \"Test Errror for the fold \"+ str(i+1)+\" is \"+str(np.round(loss_test,3))\n",
        "        plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
        "        plt.savefig('FinBERT_results/Plots/FinBERT_fold'+str(i+1)+'.png',bbox_inches='tight')\n",
        "        plt.clf()\n",
        "\n",
        "        test_loss.append(loss_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWX4awn7WwXV"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "data=[]\n",
        "data.append(train_loss)\n",
        "data.append(vald_loss)\n",
        "data.append(test_loss)\n",
        "  \n",
        "fig = plt.figure()  \n",
        "# Creating axes instance \n",
        "ax = fig.add_axes([0, 0, 1, 1]) \n",
        "  \n",
        "# Creating plot \n",
        "ax.boxplot(data)\n",
        "\n",
        "ax.set_xticklabels(['Training', 'Validation','Test']) \n",
        "\n",
        "# naming the y axis \n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title(\"Box plot for Training, Validation and Test Loss\")\n",
        "textstr ='Training Loss  : '+str(np.round(np.mean(train_loss),3))+' ('+str(np.round(np.std(train_loss),3))+')\\n'+'Validation Loss  : '+str(np.round(np.mean(vald_loss),3))+' ('+str(np.round(np.std(vald_loss),3))+')\\n'+'Test Loss  : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
        "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
        "# show plot \n",
        "plt.savefig('FinBERT_results/Plots/block_FinBERT.png',bbox_inches='tight')\n",
        "\n",
        "print('Training Loss: %.3f (%.3f)' % (np.mean(train_loss), np.std(train_loss)))\n",
        "print('Validation Loss: %.3f (%.3f)' % (np.mean(vald_loss), np.std(vald_loss)))\n",
        "print('Test Loss: %.3f (%.3f)' % (np.mean(test_loss), np.std(test_loss)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}