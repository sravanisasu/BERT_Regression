{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinBERT-10K.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravanisasu/BERT_Regression/blob/main/FinBERT_10K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VbYRib15Wad"
      },
      "source": [
        "**Setup GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6zYGrcRv_Z4",
        "outputId": "8f8a3bbc-fdca-4223-d8cf-e3c0b510a0f6"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFN5c5QkUZvQ"
      },
      "source": [
        "**Clone data from github**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDbcSqGrYEfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0418025e-1e16-4e38-8587-1e05abbffefd"
      },
      "source": [
        "!git clone https://github.com/sravanisasu/10k-sample"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '10k-sample'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 9557 (delta 1), reused 1 (delta 0), pack-reused 9548\u001b[K\n",
            "Receiving objects: 100% (9557/9557), 158.15 MiB | 20.40 MiB/s, done.\n",
            "Resolving deltas: 100% (336/336), done.\n",
            "Checking out files: 100% (10020/10020), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVmuV__8vMHR"
      },
      "source": [
        "**Necessary imports and installations for the implementation of FinBERT Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkV9qOoAZAep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc74c8c-d534-4281-cf28-eada6bf09045"
      },
      "source": [
        "% pip install sentencepiece\n",
        "% pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 5.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 23.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=fd4a3400ec7c77a4ebf5e2d5e7384e59617e3d530cd1c54214e8886e03a257ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef0WywVmvTsZ"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import os as os\n",
        "import regex as re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer,BertConfig\n",
        "from transformers import TFBertModel\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.metrics import MeanSquaredError\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhdxl-IeeuhK"
      },
      "source": [
        "**Create a FinBERT model from the transformers library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktCJhz_Il9vt",
        "outputId": "86ce7ad0-1323-438c-9b33-ff6784f6eeba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMpQFX7bXO1X",
        "outputId": "43302b40-d98c-4452-df4c-c3629bc96af4"
      },
      "source": [
        "!git clone https://github.com/sravanisasu/analyst_tone/blob/main/config.json\n",
        "!git clone https://github.com/sravanisasu/analyst_tone/blob/main/FinVocab-Uncased.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'config.json' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39rNkQ6fecnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e097153-ffed-4621-afa0-dd658cc0025a"
      },
      "source": [
        "config = BertConfig.from_pretrained(vocab_path='/content/FinVocab-Uncased.txt',pretrained_model_name_or_path='/content/config.json')\n",
        "FinBERT_model = TFBertModel.from_pretrained(config=config,pretrained_model_name_or_path='/content/drive/MyDrive/Colab Notebooks/pytorch_model.bin',from_pt=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdZmjVaXUe3Q"
      },
      "source": [
        "**Functions to preprocess input 10-K documents and output values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKJ0Aih2TEIk"
      },
      "source": [
        "######## Function to extract the input text from the files ########\n",
        "def process_inp_doc(path_file) :\n",
        "\n",
        "  file_text = open(path_file,encoding='utf8').read()\n",
        "\n",
        "  # remove punctations and digits and remove <PAGE> which was used for page number\n",
        "  file_data = re.sub(r'[\\d$%-:;!]', '', file_text)\n",
        "  file_data = re.sub(r'<PAGE>', '', file_data)\n",
        "  file_data = ''.join(file_data)\n",
        "\n",
        "  return file_data\n",
        "\n",
        "######## Function to extract the output values from the file ########\n",
        "def process_out(company_id,output_file):\n",
        "  \n",
        "  with open(output_file,'r', encoding='utf-8') as m_file :\n",
        "    for line in m_file.readlines():\n",
        "      if company_id == line.split()[1]:\n",
        "        return line.split()[0]\n",
        "    print(\"not found\")\n",
        "  return None\n",
        "\n",
        "######## Function to pre-process the documents from meta-file of a given year ########\n",
        "def pre_processing(meta_file,output_file):\n",
        "  \n",
        "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
        "    \n",
        "    year = meta_file.split('/')[3].split('.')[0]\n",
        "    dir_path = os.path.dirname(meta_file) + '/' +year+'.tok'\n",
        "    data =[]\n",
        "    \n",
        "    for line in m_file.readlines():\n",
        "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
        "\n",
        "      # get input sentences from the company document\n",
        "      inp_sentences = process_inp_doc(inp_path_file)\n",
        "    \n",
        "      # get output value for the company\n",
        "      out_values = float(process_out(line.split()[0],output_file))\n",
        "\n",
        "      #insert values into the data list\n",
        "      data.append({'text':inp_sentences,'value':out_values})\n",
        "\n",
        "  return data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AImjMKRcqhrT"
      },
      "source": [
        "**Functions to get the embeddings(token,masked,segment) and to encode the text for the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UqDE8B9e1H8"
      },
      "source": [
        "######## Function to get the encoded values ######## \n",
        "def FinBERT_encode(sentences, tokenizer, MAX_SEQ_LEN=512):\n",
        "\n",
        "  all_tokens = []\n",
        "  all_masks = []\n",
        "  all_segments = []\n",
        "  for sentence in sentences:\n",
        "    stokens = tokenizer.tokenize(sentence)\n",
        "    stokens = stokens[-MAX_SEQ_LEN+2:]\n",
        "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(stokens,)\n",
        "\n",
        "    ids = token_ids + [0] * (MAX_SEQ_LEN-len(token_ids))\n",
        "    masks = [1]*len(token_ids) + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
        "    segments = [0] * (MAX_SEQ_LEN)\n",
        "\n",
        "    all_tokens.append(ids)\n",
        "    all_masks.append(masks)\n",
        "    all_segments.append(segments)\n",
        "\n",
        "  return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc2nh_RyvagZ"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miBW5NQ7vhwh",
        "outputId": "4b8e35a7-9498-43ec-a9fe-e4c91f61accc"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  ######## extracting text and storing it in dataframes ########\n",
        "  data_train = pre_processing('/content/10k-sample/2007.meta.txt','/content/10k-sample/2007.logvol.+12.txt')\n",
        "  data_train.extend(pre_processing('/content/10k-sample/2008.meta.txt','/content/10k-sample/2008.logvol.+12.txt'))\n",
        "  data_train.extend(pre_processing('/content/10k-sample/2009.meta.txt','/content/10k-sample/2009.logvol.+12.txt'))\n",
        "  train_df = pd.DataFrame(data_train,columns=['text','value'])\n",
        "  print(\"Length of training data\",len(data_train))\n",
        "\n",
        "  data_test = pre_processing('/content/10k-sample/2010.meta.txt','/content/10k-sample/2010.logvol.+12.txt')\n",
        "  test_df = pd.DataFrame(data_test,columns=['text','value'])\n",
        "  print(\"Length of testing data\",len(data_test))\n",
        "\n",
        "  print(\"SAMPLE INPUT TEXT AND VOLATILITY VALUES\")\n",
        "  print(train_df.sample(5)[['text','value']])\n",
        "  print(test_df.sample(5)[['text','value']])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training data 7571\n",
            "Length of testing data 2439\n",
            "SAMPLE INPUT TEXT AND VOLATILITY VALUES\n",
            "                                                   text    value\n",
            "6447  item # management s discussion and analysis of... -3.29285\n",
            "4809  item # management s discussion and analysis of... -2.62700\n",
            "1171  item # management s discussion and analysis of... -3.44766\n",
            "7291  item # management s discussion and analysis of... -3.53960\n",
            "6280  item # management s discussion and analysis of... -3.39030\n",
            "                                                  text    value\n",
            "234  item # management s discussion and analysis of... -3.77937\n",
            "876  item # management s discussion and analysis of... -3.62012\n",
            "910  item # management s discussion and analysis of... -3.57330\n",
            "213  item # management s discussion and analysis of... -3.82565\n",
            "196  item # management s discussion and analysis of... -3.37386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTdv6Jj7fhmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498be3c1-8310-44fd-b924-c7a5c9097fcf"
      },
      "source": [
        "train_df = train_df.loc[train_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
        "print(train_df)\n",
        "#88.7%\n",
        "test_df = test_df.loc[test_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
        "print(test_df)\n",
        "#89.3%"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   text    value\n",
            "0     item # management s discussion and analysis of... -3.46398\n",
            "1     item # management s discussion and analysis of... -3.58048\n",
            "2     item # management s discussion and analysis of... -3.87840\n",
            "3     item # management s discussion and analysis of... -3.37969\n",
            "4     item # management s discussion and analysis of... -4.34506\n",
            "...                                                 ...      ...\n",
            "7566  item # management s discussion and analysis of... -2.75096\n",
            "7567  item # management s discussion and analysis of... -3.46372\n",
            "7568  item # management s discussion and analysis of... -2.94439\n",
            "7569  item # management s discussion and analysis of... -3.27556\n",
            "7570  item # management s discussion and analysis of... -3.33055\n",
            "\n",
            "[6717 rows x 2 columns]\n",
            "                                                   text    value\n",
            "0     item # management s discussion and analysis of... -3.87816\n",
            "1     item # management s discussion and analysis of... -3.45482\n",
            "2     item # management s discussion and analysis of... -3.78896\n",
            "3     item # management s discussion and analysis of... -3.85686\n",
            "4     item # management s discussion and analysis of... -4.71308\n",
            "...                                                 ...      ...\n",
            "2434  item # management s discussion and analysis of... -3.59450\n",
            "2435  item # management s discussion and analysis of... -3.60882\n",
            "2436  item # management s discussion and analysis of... -3.74614\n",
            "2437  item # management s discussion and analysis of... -3.51231\n",
            "2438  item # management s discussion and analysis of... -3.40038\n",
            "\n",
            "[2178 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqgmNzQ46Ya7"
      },
      "source": [
        "MAX_SEQ_LEN = 512\n",
        "\n",
        "vocab_path = '/content/FinVocab-Uncased.txt'\n",
        "######## extracting tokens from dataframes ########\n",
        "\n",
        "tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, do_basic_tokenize = True)\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  #### training \n",
        "  # input encoding\n",
        "  sentences = train_df.text.values\n",
        "  FinBERT_train_input = FinBERT_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
        "  # output values\n",
        "  FinBERT_train_output = train_df.value.values\n",
        "\n",
        "  #### test\n",
        "  # input encoding\n",
        "  sentences = test_df.text.values\n",
        "  FinBERT_test_input = FinBERT_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
        "  # output values\n",
        "  FinBERT_test_output = test_df.value.values"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJFRdYgYfux9"
      },
      "source": [
        "FinBERT_train_output = np.array(FinBERT_train_output).reshape(len(FinBERT_train_output),1)\n",
        "FinBERT_test_output = np.array(FinBERT_test_output).reshape(len(FinBERT_test_output),1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1milpeqreHi"
      },
      "source": [
        "**Function that define the model architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0uDtPqYirVK"
      },
      "source": [
        "def get_model():\n",
        "\n",
        "  input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_word_ids\")\n",
        "  input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_mask\")\n",
        "  segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"segment_ids\")\n",
        "\n",
        "  custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
        "\n",
        "  model_output = FinBERT_model(input_word_ids, input_mask, segment_ids)\n",
        "  clf_output = model_output.pooler_output\n",
        "  \n",
        "  net = tf.keras.layers.Dense(64, activation=custom_objects['leaky_relu'])(clf_output)\n",
        "\n",
        "  out = tf.keras.layers.Dense(1, activation=custom_objects['leaky_relu'])(net)\n",
        "\n",
        "  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "\n",
        "  opt = optimizers.Adam(learning_rate=0.05)\n",
        "  model.compile(optimizer=opt, loss='mse')\n",
        "\n",
        "  return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1VL1F7vsjbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b3a3f0-f6bb-4e1c-b869-801998e65868"
      },
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109751808   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 64)           49216       tf_bert_model[2][1]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            65          dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 109,801,089\n",
            "Trainable params: 109,801,089\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz3mOcj93ZrT"
      },
      "source": [
        "**Fit the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1ESuvq9eyrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b957f7-11d1-47ef-9cfe-0775645384ae"
      },
      "source": [
        "n_splits = 3\n",
        "epochs = 8\n",
        "batch_size = 4\n",
        "with tf.device('/device:GPU:0'):\n",
        "  kf = KFold(n_splits=n_splits)\n",
        "  history =[]\n",
        "  train_loss=[]\n",
        "  vald_loss=[]\n",
        "  fold = 1\n",
        "  for train_index, test_index in kf.split(FinBERT_train_input[0]):\n",
        "    \n",
        "    checkpoint_filepath = 'FinBERT_results/CheckPoints/FinBERT_checkpoint'+str(fold)\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "    train_history = model.fit(\n",
        "                              [FinBERT_train_input[0][train_index],FinBERT_train_input[1][train_index],FinBERT_train_input[2][train_index]],#input\n",
        "                              FinBERT_train_output[train_index],#output\n",
        "                              epochs=epochs, #epochs\n",
        "                              verbose=1 ,\n",
        "                              batch_size = batch_size,\n",
        "                              callbacks=[model_checkpoint_callback]\n",
        "                          )\n",
        "    model_best = tf.keras.models.load_model(checkpoint_filepath)\n",
        "    fold+=1\n",
        "    loss_T = model_best.evaluate([FinBERT_train_input[0][train_index],FinBERT_train_input[1][train_index],FinBERT_train_input[2][train_index]]\n",
        "                                       , FinBERT_train_output[train_index], verbose=0)\n",
        "    loss_V = model_best.evaluate([FinBERT_train_input[0][test_index],FinBERT_train_input[1][test_index],FinBERT_train_input[2][test_index]]\n",
        "                                      , FinBERT_train_output[test_index], verbose=0)\n",
        "    print(loss_T,loss_V)\n",
        "    train_loss.append(loss_T)\n",
        "    vald_loss.append(loss_V)\n",
        "    history.append(train_history)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "  28/1120 [..............................] - ETA: 12:40 - loss: 333.5722"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU-U3Qu_WmAi"
      },
      "source": [
        "**Plot the results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i704HZcWrdH"
      },
      "source": [
        "plt.plot(train_loss, label = \"Trainng Loss\")\n",
        "plt.plot(vald_loss, label = \"Validation Loss\")\n",
        "# naming the x axis \n",
        "plt.xlabel('Folds') \n",
        "# naming the y axis \n",
        "plt.ylabel('Error') \n",
        "# function to show the plot \n",
        "plt.legend()\n",
        "plt.savefig('FinBERT_results/Plots/FinBERT_loss_check.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yiCNOvgWt2k"
      },
      "source": [
        "test_loss = []\n",
        "with tf.device('/device:GPU:0'):\n",
        "    \n",
        "    for i in range(n_splits):\n",
        "\n",
        "        checkpoint_filepath = 'FinBERT_results/CheckPoints/FinBERT_checkpoint'+str(i+1) \n",
        "        best_model = tf.keras.models.load_model(\n",
        "                                                checkpoint_filepath, custom_objects=None, compile=True, options=None\n",
        "                                                )\n",
        "        predicted = best_model.predict(FinBERT_test_input[0:50])\n",
        "        \n",
        "        loss_test = best_model.evaluate([FinBERT_test_input[0],FinBERT_test_input[1],FinBERT_test_input[2]]\n",
        "                                          , FinBERT_test_output, verbose=0)\n",
        "        print(\"Test Errror for the fold \",i+1,\" is\",loss_test )\n",
        "        \n",
        "        \n",
        "        plt.plot(predicted[0:50], label = \"Predicted Values\")  \n",
        "        plt.plot(FinBERT_test_output[0:50], label = \"Actual Values\")\n",
        "        # naming the x axis \n",
        "        plt.xlabel('Test Samples') \n",
        "        # naming the y axis \n",
        "        plt.ylabel('Output Values') \n",
        "        # function to show the plot \n",
        "        plt.legend()\n",
        "        textstr = \"Test Errror for the fold \"+ str(i+1)+\" is \"+str(np.round(loss_test,3))\n",
        "        plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
        "        plt.savefig('FinBERT_results/Plots/FinBERT_fold'+str(i+1)+'.png',bbox_inches='tight')\n",
        "        plt.clf()\n",
        "\n",
        "        test_loss.append(loss_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWX4awn7WwXV"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "data=[]\n",
        "data.append(train_loss)\n",
        "data.append(vald_loss)\n",
        "data.append(test_loss)\n",
        "  \n",
        "fig = plt.figure()  \n",
        "# Creating axes instance \n",
        "ax = fig.add_axes([0, 0, 1, 1]) \n",
        "  \n",
        "# Creating plot \n",
        "ax.boxplot(data)\n",
        "\n",
        "ax.set_xticklabels(['Training', 'Validation','Test']) \n",
        "\n",
        "# naming the y axis \n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title(\"Box plot for Training, Validation and Test Loss\")\n",
        "textstr ='Training Loss  : '+str(np.round(np.mean(train_loss),3))+' ('+str(np.round(np.std(train_loss),3))+')\\n'+'Validation Loss  : '+str(np.round(np.mean(vald_loss),3))+' ('+str(np.round(np.std(vald_loss),3))+')\\n'+'Test Loss  : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
        "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
        "# show plot \n",
        "plt.savefig('FinBERT_results/Plots/block_FinBERT.png',bbox_inches='tight')\n",
        "\n",
        "print('Training Loss: %.3f (%.3f)' % (np.mean(train_loss), np.std(train_loss)))\n",
        "print('Validation Loss: %.3f (%.3f)' % (np.mean(vald_loss), np.std(vald_loss)))\n",
        "print('Test Loss: %.3f (%.3f)' % (np.mean(test_loss), np.std(test_loss)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}