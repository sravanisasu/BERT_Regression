{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Necessary imports and installations for the implementation of BERT Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import os as os\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a BERT layer from the tensorflow-hub library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to preprocess input 10-K documents and output values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to extract the input text from the files ########\n",
    "def process_inp_doc(path_file) :\n",
    "\n",
    "  file_text = open(path_file,encoding='utf8').read()\n",
    "\n",
    "  # remove punctations and digits and remove <PAGE> which was used for page number\n",
    "  file_data = re.sub(r'[\\d$%-:;!]', '', file_text)\n",
    "  file_data = re.sub(r'<PAGE>', '', file_data)\n",
    "  file_data = ''.join(file_data)\n",
    "\n",
    "  return file_data\n",
    "\n",
    "######## Function to extract the output values from the file ########\n",
    "def process_out(company_id,output_file):\n",
    "  \n",
    "  with open(output_file,'r', encoding='utf-8') as m_file :\n",
    "    for line in m_file.readlines():\n",
    "      if company_id == line.split()[1]:\n",
    "        return line.split()[0]\n",
    "    print(\"not found\")\n",
    "  return None\n",
    "\n",
    "######## Function to pre-process the documents from meta-file of a given year ########\n",
    "def pre_processing(meta_file,output_file):\n",
    "  \n",
    "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
    "    \n",
    "    year = meta_file.split('/')[1].split('.')[0]\n",
    "    dir_path = os.path.dirname(meta_file) + '/' +year+'.tok'\n",
    "    data =[]\n",
    "    for line in m_file.readlines():\n",
    "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
    "\n",
    "      # get input sentences from the company document\n",
    "      inp_sentences = process_inp_doc(inp_path_file)\n",
    "    \n",
    "      # get output value for the company\n",
    "      out_values = float(process_out(line.split()[0],output_file))\n",
    "\n",
    "      #insert values into the data list\n",
    "      data.append({'text':inp_sentences,'value':out_values})\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to get the embeddings(token,masked,segment) and to encode the text for the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Function to get the encoded values ######## \n",
    "def bert_encode(sentences, tokenizer, MAX_SEQ_LEN=512):\n",
    "\n",
    "  all_tokens = []\n",
    "  all_masks = []\n",
    "  all_segments = []\n",
    "  for sentence in sentences:\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = stokens[-MAX_SEQ_LEN+2:]\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(stokens,)\n",
    "\n",
    "    ids = token_ids + [0] * (MAX_SEQ_LEN-len(token_ids))\n",
    "    masks = [1]*len(token_ids) + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    segments = [0] * (MAX_SEQ_LEN)\n",
    "\n",
    "    all_tokens.append(ids)\n",
    "    all_masks.append(masks)\n",
    "    all_segments.append(segments)\n",
    "\n",
    "  return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "  ######## extracting text and storing it in dataframes ########\n",
    "  data_train = pre_processing('10k-sample/2007.meta.txt','10k-sample/2007.logvol.+12.txt')\n",
    "  data_train.extend(pre_processing('10k-sample/2008.meta.txt','10k-sample/2008.logvol.+12.txt'))\n",
    "  data_train.extend(pre_processing('10k-sample/2009.meta.txt','10k-sample/2009.logvol.+12.txt'))\n",
    "  train_df = pd.DataFrame(data_train,columns=['text','value'])\n",
    "  print(\"Length of training data\",len(data_train))\n",
    "\n",
    "  data_test = pre_processing('10k-sample/2010.meta.txt','10k-sample/2010.logvol.+12.txt')\n",
    "  test_df = pd.DataFrame(data_test,columns=['text','value'])\n",
    "  print(\"Length of testing data\",len(data_test))\n",
    "\n",
    "  print(\"SAMPLE INPUT TEXT AND VOLATILITY VALUES\")\n",
    "  print(train_df.sample(5)[['text','value']])\n",
    "  print(test_df.sample(5)[['text','value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.loc[train_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "print(train_df)\n",
    "#88.7%\n",
    "test_df = test_df.loc[test_df[\"text\"].apply(lambda x: x.split().__len__())>256]\n",
    "print(test_df)\n",
    "#89.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## extracting tokens from dataframes ########\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  \n",
    "  #### training \n",
    "  # input encoding\n",
    "  sentences = train_df.text.values\n",
    "  bert_train_input = bert_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "  # output values\n",
    "  bert_train_output = train_df.value.values\n",
    "\n",
    "  #### test\n",
    "  # input encoding\n",
    "  sentences = test_df.text.values\n",
    "  bert_test_input = bert_encode(sentences, tokenizer, MAX_SEQ_LEN)\n",
    "  # output values\n",
    "  bert_test_output = test_df.value.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "bert_train_output = np.array(bert_train_output).reshape(len(bert_train_output),1)\n",
    "bert_test_output = np.array(bert_test_output).reshape(len(bert_test_output),1)\n",
    "output = np.concatenate((bert_train_output, bert_test_output))\n",
    "output = scaler.fit_transform(output)\n",
    "bert_train_output = output[:len(bert_train_input[0])]\n",
    "bert_test_output = output[-len(bert_test_input[0]):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function that define the model architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "  input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_word_ids\")\n",
    "  input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_mask\")\n",
    "  segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"segment_ids\")\n",
    "\n",
    "  custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
    "\n",
    "  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "  clf_output = pooled_output\n",
    "  \n",
    "  net = tf.keras.layers.Dense(64, activation=custom_objects['leaky_relu'])(clf_output)\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = Flatten()(net)\n",
    "  out = tf.keras.layers.Dense(1, activation=custom_objects['leaky_relu'])(net)\n",
    "\n",
    "  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "\n",
    "  opt = optimizers.Adam(learning_rate=0.05)\n",
    "  model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "epochs = 50\n",
    "with tf.device('/device:GPU:0'):\n",
    "  kf = KFold(n_splits=n_splits)\n",
    "  history =[]\n",
    "  train_loss=[]\n",
    "  vald_loss=[]\n",
    "  fold = 1\n",
    "  for train_index, test_index in kf.split(bert_train_input[0]):\n",
    "        \n",
    "    print(\"Train Index: \", train_index, \"\\n\")\n",
    "    print(\"Validation Index: \", test_index)\n",
    "    \n",
    "    checkpoint_filepath = 'Bert_results/CheckPoints/bert_checkpoint'+str(fold)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "    train_history = model.fit(\n",
    "                              [bert_train_input[0][train_index],bert_train_input[1][train_index],bert_train_input[2][train_index]],#input\n",
    "                              bert_train_output[train_index],#output\n",
    "                              epochs=epochs, #epochs\n",
    "                              verbose=1,\n",
    "                              callbacks=[model_checkpoint_callback]\n",
    "                          )\n",
    "    model_best = tf.keras.models.load_model(checkpoint_filepath)\n",
    "    fold+=1\n",
    "    loss_T = model_best.evaluate([bert_train_input[0][train_index],bert_train_input[1][train_index],bert_train_input[2][train_index]]\n",
    "                                       , bert_train_output[train_index], verbose=0)\n",
    "    loss_V = model_best.evaluate([bert_train_input[0][test_index],bert_train_input[1][test_index],bert_train_input[2][test_index]]\n",
    "                                      , bert_train_output[test_index], verbose=0)\n",
    "    print(loss_T,loss_V)\n",
    "    train_loss.append(loss_T)\n",
    "    vald_loss.append(loss_V)\n",
    "    history.append(train_history)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot all the folds together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label = \"Trainng Loss\")\n",
    "plt.plot(vald_loss, label = \"Validation Loss\")\n",
    "# naming the x axis \n",
    "plt.xlabel('Folds') \n",
    "# naming the y axis \n",
    "plt.ylabel('Error') \n",
    "# function to show the plot \n",
    "plt.legend()\n",
    "plt.savefig('Bert_results/Plots/bert_loss_check.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict the Model from the checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = []\n",
    "with tf.device('/device:GPU:0'):\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "\n",
    "        checkpoint_filepath = 'Bert_results/CheckPoints/bert_checkpoint'+str(i+1) \n",
    "        best_model = tf.keras.models.load_model(\n",
    "                                                checkpoint_filepath, custom_objects=None, compile=True, options=None\n",
    "                                                )\n",
    "        predicted = best_model.predict(bert_test_input[0:50])\n",
    "        \n",
    "        loss_test = best_model.evaluate([bert_test_input[0],bert_test_input[1],bert_test_input[2]]\n",
    "                                          , bert_test_output, verbose=0)\n",
    "        print(\"Test Errror for the fold \",i+1,\" is\",loss_test )\n",
    "        \n",
    "        \n",
    "        plt.plot(predicted[0:50], label = \"Predicted Values\")  \n",
    "        plt.plot(bert_test_output[0:50], label = \"Actual Values\")\n",
    "        # naming the x axis \n",
    "        plt.xlabel('Test Samples') \n",
    "        # naming the y axis \n",
    "        plt.ylabel('Output Values') \n",
    "        # function to show the plot \n",
    "        plt.legend()\n",
    "        textstr = \"Test Errror for the fold \"+ str(i+1)+\" is \"+str(np.round(loss_test,3))\n",
    "        plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
    "        plt.savefig('Bert_results/Plots/bert_fold'+str(i+1)+'.png',bbox_inches='tight')\n",
    "        plt.clf()\n",
    "\n",
    "        test_loss.append(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "data=[]\n",
    "data.append(train_loss)\n",
    "data.append(vald_loss)\n",
    "data.append(test_loss)\n",
    "  \n",
    "fig = plt.figure()  \n",
    "# Creating axes instance \n",
    "ax = fig.add_axes([0, 0, 1, 1]) \n",
    "  \n",
    "# Creating plot \n",
    "ax.boxplot(data)\n",
    "\n",
    "ax.set_xticklabels(['Training', 'Validation','Test']) \n",
    "\n",
    "# naming the y axis \n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title(\"Box plot for Training, Validation and Test Loss\")\n",
    "textstr ='Training Loss  : '+str(np.round(np.mean(train_loss),3))+' ('+str(np.round(np.std(train_loss),3))+')\\n'+'Validation Loss  : '+str(np.round(np.mean(vald_loss),3))+' ('+str(np.round(np.std(vald_loss),3))+')\\n'+'Test Loss  : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
    "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
    "# show plot \n",
    "plt.savefig('Bert_results/Plots/block_bert.png',bbox_inches='tight')\n",
    "\n",
    "print('Training Loss: %.3f (%.3f)' % (np.mean(train_loss), np.std(train_loss)))\n",
    "print('Validation Loss: %.3f (%.3f)' % (np.mean(vald_loss), np.std(vald_loss)))\n",
    "print('Test Loss: %.3f (%.3f)' % (np.mean(test_loss), np.std(test_loss)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
