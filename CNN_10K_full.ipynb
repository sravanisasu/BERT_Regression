{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing the required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/catlabuser2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building embedding for the words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding bin file\n",
    "embed_file = \"10k-sample/sim.expand.200d.vec\"\n",
    "\n",
    "#Define Hyper parameters\n",
    "max_inp_len = 20000\n",
    "# the dimension of vectors to be used\n",
    "embed_dim = 200\n",
    "rounding = 6\n",
    "# filter sizes of the different conv layers \n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 1\n",
    "pool_size = 199\n",
    "# dropout probability\n",
    "drop = 0.5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of embedding dictionary 70429\n"
     ]
    }
   ],
   "source": [
    "#define embedding dictionary and embed matrix for the vocabulary\n",
    "embeddings_dic = dict()\n",
    "f = open(embed_file,encoding='utf8')\n",
    "\n",
    "with open(embed_file, 'r', encoding='utf-8') as e_file:\n",
    "    for line in e_file:\n",
    "        splitlines = line.split()\n",
    "        word = splitlines[0].strip()\n",
    "        coefs = np.asarray(splitlines[1:], dtype='float32')\n",
    "        embeddings_dic[word] = coefs\n",
    "\n",
    "print(\"length of embedding dictionary\",len(embeddings_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix dimension 70429 200\n",
      "no of token in the tokenizer 70429\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(embeddings_dic.keys())\n",
    "embed_token = Tokenizer()\n",
    "embed_token.fit_on_texts(embeddings_dic.keys())\n",
    "embedding_matrix = np.zeros((vocabulary_size, embed_dim))\n",
    "for word, index in embed_token.word_index.items():\n",
    "  embedding_matrix[index] = embeddings_dic.get(word)\n",
    "print(\"embedding_matrix dimension\",len(embedding_matrix),len(embedding_matrix[0]))\n",
    "print(\"no of token in the tokenizer\",len(embed_token.word_index) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functions to pre process input and output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pre process the document\n",
    "def process_doc(path_file,embed_token) :\n",
    "\n",
    "  #tokenizing the words \n",
    "  with open(path_file,'r', encoding='utf-8') as tok_file :\n",
    "    file_words = list(tok_file)[0].split()\n",
    "    \n",
    "  #removing the stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  filtered_words = []  \n",
    "  for word in file_words: \n",
    "      if word not in stop_words and word.isalpha(): \n",
    "          filtered_words.append(word)\n",
    "\n",
    "  # applying stemming using PorterStemmer\n",
    "\n",
    "  p_stemmer = PorterStemmer()\n",
    "  stem_words=[]\n",
    "  for word in filtered_words:\n",
    "    stem_words.append(p_stemmer.stem(word))\n",
    "    \n",
    "  #tokenizing the words using the embed token\n",
    "  tokens=[]\n",
    "  for word in stem_words:\n",
    "    try:\n",
    "      tokens.append(embed_token.word_index[word])\n",
    "    except:\n",
    "      tokens.append(1)\n",
    "\n",
    "  if len(tokens) < max_inp_len:\n",
    "    tokens.extend([0]*(max_inp_len-len(tokens)))\n",
    "  else:\n",
    "    tokens = tokens[:max_inp_len]\n",
    "    \n",
    "  return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output dataset\n",
    "def output_data(company_id, out_path_file):\n",
    "  with open(out_path_file,'r', encoding='utf-8') as out_file :\n",
    "    for line in out_file.readlines():\n",
    "      if company_id == line.split()[1]:\n",
    "        return line.split()[0]\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(meta_file,output_file):\n",
    "\n",
    "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
    "    \n",
    "    year = meta_file.split('/')[2].split('.')[0]\n",
    "    dir_path = os.path.dirname(meta_file).split('/')[0] + '/all.tok/' +year+'.tok'\n",
    "    data =[]\n",
    "\n",
    "    for line in m_file.readlines():\n",
    "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
    "      \n",
    "      # get input tokens from the company document\n",
    "      inp_tokens = process_doc(inp_path_file,embed_token)\n",
    "      \n",
    "      # get output value for the company\n",
    "      out_values = output_data(line.split()[0],output_file)\n",
    "\n",
    "      #insert values into the data list\n",
    "      data.append({'token':inp_tokens,'value':out_values})\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(max_inp_len,vocabulary_size,embed_dim,filter_sizes,num_filters,pool_size,drop,learning_rate):\n",
    "    \n",
    "    # input and embedding matrix\n",
    "    inputs = Input(shape=(max_inp_len,))\n",
    "    embedding = Embedding(vocabulary_size, embed_dim, weights=[embedding_matrix],trainable = False)(inputs)\n",
    "    \n",
    "    custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
    "    \n",
    "    # channel 1 convolution and local max-pooling\n",
    "    convolution_1 = Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation=custom_objects['leaky_relu'])(embedding)\n",
    "    pool_1 = MaxPooling1D(pool_size=pool_size)(convolution_1)\n",
    "    \n",
    "    # channel 2 convolution and local max-pooling\n",
    "    convolution_4 = Conv1D(filters=num_filters, kernel_size=filter_sizes[1], activation=custom_objects['leaky_relu'])(embedding)\n",
    "    pool_2 = MaxPooling1D(pool_size=pool_size)(convolution_4)\n",
    "    \n",
    "    # channel 3 convolution and local max-pooling\n",
    "    convolution_5 = Conv1D(filters=num_filters, kernel_size=filter_sizes[2], activation=custom_objects['leaky_relu'])(embedding)\n",
    "    pool_3 = MaxPooling1D(pool_size=pool_size)(convolution_5)\n",
    "    \n",
    "    # merge and dropout\n",
    "    merged = concatenate([pool_1,pool_2,pool_3],axis=1)\n",
    "    drop_out = Dropout(drop)(merged)\n",
    "    flat = Flatten()(drop_out)\n",
    "    \n",
    "    # 2 fully connected layers\n",
    "    dense1 = Dense(100, activation=custom_objects['leaky_relu'])(flat)\n",
    "    outputs = Dense(1, activation=custom_objects['leaky_relu'])(dense1)\n",
    "    model = Model(inputs=[inputs], outputs=outputs)\n",
    "    \n",
    "    opt = optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20000, 200)   14085800    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 19998, 1)     601         embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 19997, 1)     801         embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 19996, 1)     1001        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 100, 1)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 100, 1)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 100, 1)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 300, 1)       0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300, 1)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 300)          0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          30100       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            101         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 14,118,404\n",
      "Trainable params: 32,604\n",
      "Non-trainable params: 14,085,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(max_inp_len,vocabulary_size,embed_dim,filter_sizes,num_filters,pool_size,drop,learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data extraction and Fitting the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;6d51ea1624a031b0;/job:localhost/replica:0/task:0/device:GPU:0;edge_32_model/embedding/embedding_lookup;0:0\n\t [[{{node model/embedding/embedding_lookup/_10}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_874]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7e9114c1ed0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m                                                     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                                     save_best_only=True)\n\u001b[0;32m---> 46\u001b[0;31m             train_history = model.fit(\n\u001b[0m\u001b[1;32m     47\u001b[0m                                   \u001b[0mCNN_train_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                   \u001b[0mCNN_train_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m#output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;6d51ea1624a031b0;/job:localhost/replica:0/task:0/device:GPU:0;edge_32_model/embedding/embedding_lookup;0:0\n\t [[{{node model/embedding/embedding_lookup/_10}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_874]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "test_loss_all_years = []\n",
    "data = []\n",
    "n_splits = 5\n",
    "for year in range(2008,2014):\n",
    "\n",
    "    ######## extracting text and storing it in dataframes ########\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        data_train = pre_processing('10k-sample/all.meta/'+str(year-3)+'.meta.txt','10k-sample/all.logfama/'+str(year-3)+'.logfama.txt')\n",
    "        data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-2)+'.meta.txt','10k-sample/all.logfama/'+str(year-2)+'.logfama.txt'))\n",
    "        data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-1)+'.meta.txt','10k-sample/all.logfama/'+str(year-1)+'.logfama.txt'))\n",
    "        train_df = pd.DataFrame(data_train,columns=['token','value'])\n",
    "        \n",
    "        data_test = pre_processing('10k-sample/all.meta/'+str(year)+'.meta.txt','10k-sample/all.logfama/'+str(year)+'.logfama.txt')\n",
    "        test_df = pd.DataFrame(data_test,columns=['token','value'])\n",
    "        data.append({'year':year,'train_df_length':len(data_train),'test_df_length':len(data_test)})\n",
    "    \n",
    "    ######## reshapping data to required format ########\n",
    "    CNN_train_input = train_df.token.values\n",
    "    CNN_train_output = [ float(x) for x in train_df.value.values ]\n",
    "    CNN_test_input = test_df.token.values\n",
    "    CNN_test_output = [ float(x) for x in test_df.value.values ]\n",
    "    CNN_train_output = np.array(CNN_train_output).reshape(len(CNN_train_output),1)\n",
    "    CNN_test_output = np.array(CNN_test_output).reshape(len(CNN_test_output),1)\n",
    "    \n",
    "    CNN_train_input = np.stack(CNN_train_input)\n",
    "    CNN_test_input = np.stack(CNN_test_input)\n",
    "\n",
    "    ######## Kfold training and saving checkpoints ########\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "        history =[]\n",
    "        train_loss=[]\n",
    "        vald_loss=[]\n",
    "        test_loss = []\n",
    "        fold = 1\n",
    "        \n",
    "        for train_index, test_index in kf.split(CNN_train_input):\n",
    "            \n",
    "            checkpoint_filepath = 'Results/CNN_results/CheckPoints/'+str(year)+'CNN_checkpoint'+str(fold)\n",
    "            model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                    filepath=checkpoint_filepath,\n",
    "                                                    save_weights_only=False,\n",
    "                                                    monitor='loss',\n",
    "                                                    mode='min',\n",
    "                                                    save_best_only=True)\n",
    "            train_history = model.fit(\n",
    "                                  CNN_train_input[train_index],\n",
    "                                  CNN_train_output[train_index],#output\n",
    "                                  epochs=epochs, #epochs\n",
    "                                  verbose=1,\n",
    "                                  callbacks=[model_checkpoint_callback]\n",
    "                              )\n",
    "            model_best = tf.keras.models.load_model(checkpoint_filepath)\n",
    "            fold+=1\n",
    "            loss_T = model_best.evaluate(CNN_train_input[train_index],CNN_train_output[train_index], verbose=0)\n",
    "            loss_V = model_best.evaluate(CNN_train_input[test_index],CNN_train_output[test_index], verbose=0)\n",
    "            loss_test = model_best.evaluate(CNN_test_input,CNN_test_output, verbose=0)\n",
    "\n",
    "            train_loss.append(loss_T)\n",
    "            vald_loss.append(loss_V)\n",
    "            history.append(train_history)\n",
    "            test_loss.append(loss_test)\n",
    "            \n",
    "            \n",
    "        test_loss_all_years.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(data,columns=['year','train_df_length','test_df_length'])\n",
    "stats_df.to_csv('Loss_values/CNN_stats.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "data=[test_loss]\n",
    "  \n",
    "fig = plt.figure()  \n",
    "# Creating axes instance \n",
    "ax = fig.add_axes([0, 0, 1, 1]) \n",
    "  \n",
    "# Creating plot \n",
    "ax.boxplot(data)\n",
    "years = [year for year in range(2008,2014)]\n",
    "ax.set_xticklabels([year for year in range(2008,2014)]) \n",
    "\n",
    "# naming the y axis \n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title(\"Box plot for Test Loss\")\n",
    "textstr ='Test Loss for CNN : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
    "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
    "# show plot \n",
    "plt.savefig('Plots/block_plot_CNN.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = []\n",
    "for year_losses,year in zip(test_loss,years) :\n",
    "    loss_data.append({'year':year,'value':year_losses})\n",
    "loss_data_df = pd.DataFrame(loss_data,columns=['year','value'])\n",
    "loss_data_df.to_csv('Loss_values/CNN_Loss.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
