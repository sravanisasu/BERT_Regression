{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catlab/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing the required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/catlab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.metrics import RootMeanSquaredError\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building embedding for the words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding bin file\n",
    "embed_file = \"10k-sample/sim.expand.200d.vec\"\n",
    "\n",
    "#Define Hyper parameters\n",
    "max_inp_len = 20000\n",
    "# the dimension of vectors to be used\n",
    "embed_dim = 200\n",
    "rounding = 6\n",
    "# filter sizes of the different conv layers \n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 1\n",
    "pool_size = 199\n",
    "# dropout probability\n",
    "drop = 0.5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of embedding dictionary 70429\n"
     ]
    }
   ],
   "source": [
    "#define embedding dictionary and embed matrix for the vocabulary\n",
    "embeddings_dic = dict()\n",
    "f = open(embed_file,encoding='utf8')\n",
    "with open(embed_file, 'r', encoding='utf-8') as e_file:\n",
    "  for line in e_file:\n",
    "    splitlines = line.split()\n",
    "    word = splitlines[0].strip()\n",
    "    coefs = np.asarray(splitlines[1:], dtype='float32')\n",
    "    embeddings_dic[word] = coefs\n",
    "\n",
    "print(\"length of embedding dictionary\",len(embeddings_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix dimension 70429 200\n",
      "no of token in the tokenizer 70429\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(embeddings_dic.keys())\n",
    "embed_token = Tokenizer()\n",
    "embed_token.fit_on_texts(embeddings_dic.keys())\n",
    "embedding_matrix = np.zeros((vocabulary_size, embed_dim))\n",
    "for word, index in embed_token.word_index.items():\n",
    "    embedding_matrix[index] = embeddings_dic.get(word)\n",
    "print(\"embedding_matrix dimension\",len(embedding_matrix),len(embedding_matrix[0]))\n",
    "print(\"no of token in the tokenizer\",len(embed_token.word_index) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functions to pre process input and output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pre process the document\n",
    "def process_doc(path_file,embed_token) :\n",
    "\n",
    "  #tokenizing the words \n",
    "  with open(path_file,'r', encoding='utf-8') as tok_file :\n",
    "    file_words = list(tok_file)[0].split()\n",
    "    \n",
    "  #removing the stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  filtered_words = []  \n",
    "  for word in file_words: \n",
    "      if word not in stop_words and word.isalpha(): \n",
    "          filtered_words.append(word)\n",
    "\n",
    "  # applying stemming using PorterStemmer\n",
    "\n",
    "  p_stemmer = PorterStemmer()\n",
    "  stem_words=[]\n",
    "  for word in filtered_words:\n",
    "    stem_words.append(p_stemmer.stem(word))\n",
    "    \n",
    "  #tokenizing the words using the embed token\n",
    "  tokens=[]\n",
    "  for word in stem_words:\n",
    "    try:\n",
    "      tokens.append(embed_token.word_index[word])\n",
    "    except:\n",
    "      tokens.append(1)\n",
    "\n",
    "  if len(tokens) < max_inp_len:\n",
    "    tokens.extend([0]*(max_inp_len-len(tokens)))\n",
    "  else:\n",
    "    tokens = tokens[:max_inp_len]\n",
    "    \n",
    "  return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output dataset\n",
    "def output_data(company_id, out_path_file):\n",
    "  with open(out_path_file,'r', encoding='utf-8') as out_file :\n",
    "    for line in out_file.readlines():\n",
    "      if company_id == line.split()[1]:\n",
    "        return line.split()[0]\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(meta_file,output_file):\n",
    "\n",
    "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
    "    \n",
    "    year = meta_file.split('/')[2].split('.')[0]\n",
    "    dir_path = os.path.dirname(meta_file).split('/')[0] + '/all.tok/' +year+'.tok'\n",
    "    data =[]\n",
    "\n",
    "    for line in m_file.readlines():\n",
    "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
    "      \n",
    "      # get input tokens from the company document\n",
    "      inp_tokens = process_doc(inp_path_file,embed_token)\n",
    "      \n",
    "      # get output value for the company\n",
    "      out_values = output_data(line.split()[0],output_file)\n",
    "\n",
    "      #insert values into the data list\n",
    "      data.append({'token':inp_tokens,'value':out_values})\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(max_inp_len,vocabulary_size,embed_dim,filter_sizes,num_filters,pool_size,drop,learning_rate):\n",
    "    \n",
    "    # input and embedding matrix\n",
    "    inputs = Input(shape=(max_inp_len,))\n",
    "    embedding = Embedding(vocabulary_size, embed_dim, weights=[embedding_matrix],trainable = False)(inputs)\n",
    "    \n",
    "    custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
    "    \n",
    "    # channel 1 convolution and local max-pooling\n",
    "    convolution_1 = Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation=custom_objects['leaky_relu'])(embedding)\n",
    "    pool_1 = MaxPooling1D(pool_size=pool_size)(convolution_1)\n",
    "    \n",
    "    # channel 2 convolution and local max-pooling\n",
    "    convolution_4 = Conv1D(filters=num_filters, kernel_size=filter_sizes[1], activation=custom_objects['leaky_relu'])(embedding)\n",
    "    pool_2 = MaxPooling1D(pool_size=pool_size)(convolution_4)\n",
    "    \n",
    "    # channel 3 convolution and local max-pooling\n",
    "    convolution_5 = Conv1D(filters=num_filters, kernel_size=filter_sizes[2], activation=custom_objects['leaky_relu'])(embedding)\n",
    "    pool_3 = MaxPooling1D(pool_size=pool_size)(convolution_5)\n",
    "    \n",
    "    # merge and dropout\n",
    "    merged = concatenate([pool_1,pool_2,pool_3],axis=1)\n",
    "    drop_out = Dropout(drop)(merged)\n",
    "    flat = Flatten()(drop_out)\n",
    "    \n",
    "    # 2 fully connected layers\n",
    "    dense1 = Dense(100, activation=custom_objects['leaky_relu'])(flat)\n",
    "    outputs = Dense(1, activation=custom_objects['leaky_relu'])(dense1)\n",
    "    model = Model(inputs=[inputs], outputs=outputs)\n",
    "    \n",
    "    opt = optimizers.SGD()\n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/catlab/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20000, 200)   14085800    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 19998, 1)     601         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 19997, 1)     801         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 19996, 1)     1001        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 100, 1)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 100, 1)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 100, 1)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300, 1)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 300, 1)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 300)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          30100       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            101         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,118,404\n",
      "Trainable params: 32,604\n",
      "Non-trainable params: 14,085,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(max_inp_len,vocabulary_size,embed_dim,filter_sizes,num_filters,pool_size,drop,learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data extraction and Fitting the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "test_loss_all_years = []\n",
    "train_loss_all_years = []\n",
    "val_loss_all_years = []\n",
    "history_all_years = []\n",
    "data = []\n",
    "n_splits = 5\n",
    "#for year in range(2008,2014):\n",
    "year = 2008   \n",
    "######## extracting text and storing it in dataframes ########      \n",
    "data_train = pre_processing('10k-sample/all.meta/'+str(year-3)+'.meta.txt','10k-sample/all.logfama/'+str(year-3)+'.logfama.txt')\n",
    "data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-2)+'.meta.txt','10k-sample/all.logfama/'+str(year-2)+'.logfama.txt'))\n",
    "data_train.extend(pre_processing('10k-sample/all.meta/'+str(year-1)+'.meta.txt','10k-sample/all.logfama/'+str(year-1)+'.logfama.txt'))\n",
    "train_df = pd.DataFrame(data_train,columns=['token','value'])\n",
    "\n",
    "data_test = pre_processing('10k-sample/all.meta/'+str(year)+'.meta.txt','10k-sample/all.logfama/'+str(year)+'.logfama.txt')\n",
    "test_df = pd.DataFrame(data_test,columns=['token','value'])\n",
    "data.append({'year':year,'train_df_length':len(data_train),'test_df_length':len(data_test)})\n",
    "\n",
    "######## reshapping data to required format ########\n",
    "CNN_train_input = train_df.token.values\n",
    "CNN_train_output = [ float(x) for x in train_df.value.values ]\n",
    "CNN_test_input = test_df.token.values\n",
    "CNN_test_output = [ float(x) for x in test_df.value.values ]\n",
    "CNN_train_output = np.array(CNN_train_output).reshape(len(CNN_train_output),1)\n",
    "CNN_test_output = np.array(CNN_test_output).reshape(len(CNN_test_output),1)\n",
    "\n",
    "CNN_train_input = np.stack(CNN_train_input)\n",
    "CNN_test_input = np.stack(CNN_test_input)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "CNN_train_output = np.array(CNN_train_output).reshape(len(CNN_train_output),1)\n",
    "CNN_test_output = np.array(CNN_test_output).reshape(len(CNN_test_output),1)\n",
    "output = np.concatenate((CNN_train_output, CNN_test_output))\n",
    "output = scaler.fit_transform(output)\n",
    "CNN_train_output = output[:len(CNN_train_input)]\n",
    "CNN_test_output = output[-len(CNN_test_input):]\n",
    "\n",
    "######## Kfold training and saving checkpoints ########\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    history =[]\n",
    "    train_loss=[]\n",
    "    vald_loss=[]\n",
    "    test_loss = []\n",
    "    fold = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(CNN_train_input):\n",
    "\n",
    "        checkpoint_filepath = 'Results/CNN_results_min/CheckPoints/'+str(year)+'CNN_checkpoint'+str(fold)\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                filepath=checkpoint_filepath,\n",
    "                                                save_weights_only=False,\n",
    "                                                monitor='loss',\n",
    "                                                mode='min',\n",
    "                                                save_best_only=True)\n",
    "        train_history = model.fit(\n",
    "                              CNN_train_input[train_index],\n",
    "                              CNN_train_output[train_index],#output\n",
    "                              epochs=epochs, #epochs\n",
    "                              verbose=1,\n",
    "                              callbacks=[model_checkpoint_callback]\n",
    "                          )\n",
    "        model_best = tf.keras.models.load_model(checkpoint_filepath)\n",
    "        fold+=1\n",
    "        loss_T = model_best.evaluate(CNN_train_input[train_index],CNN_train_output[train_index], verbose=0)\n",
    "        loss_V = model_best.evaluate(CNN_train_input[test_index],CNN_train_output[test_index], verbose=0)\n",
    "        loss_test = model_best.evaluate(CNN_test_input,CNN_test_output, verbose=0)\n",
    "\n",
    "        train_loss.append(loss_T)\n",
    "        vald_loss.append(loss_V)\n",
    "        history.append(train_history)\n",
    "        test_loss.append(loss_test)\n",
    "\n",
    "    test_loss_all_years.append(test_loss)\n",
    "    train_loss_all_years.append(train_loss)\n",
    "    val_loss_all_years.append(vald_loss)\n",
    "    history_all_years.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(data,columns=['year','train_df_length','test_df_length'])\n",
    "stats_df.to_csv('Loss_values/CNN_stats_minmax.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "data=[test_loss]\n",
    "  \n",
    "fig = plt.figure()  \n",
    "# Creating axes instance \n",
    "ax = fig.add_axes([0, 0, 1, 1]) \n",
    "  \n",
    "# Creating plot \n",
    "ax.boxplot(data)\n",
    "years = [year for year in range(2008,2014)]\n",
    "ax.set_xticklabels([year for year in range(2008,2014)]) \n",
    "\n",
    "# naming the y axis \n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title(\"Box plot for Test Loss\")\n",
    "textstr ='Test Loss for CNN : '+str(np.round(np.mean(test_loss),3))+' ('+str(np.round(np.std(test_loss),3))+')'\n",
    "plt.gcf().text(0, -0.25, textstr, fontsize=14)\n",
    "# show plot \n",
    "plt.savefig('Plots/block_plot_CNN_minmax.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = []\n",
    "train_data = []\n",
    "vald_data = []\n",
    "for year_loss_test,year_loss_train,year_loss_vald,year in zip(test_loss_all_years,train_loss_all_years,val_loss_all_years,years) :\n",
    "    loss_data.append({'year':year,'value':year_loss_test})\n",
    "    train_data.append({'year':year,'value':year_loss_train})\n",
    "    vald_data.append({'year':year,'value':year_loss_vald})\n",
    "    \n",
    "loss_data_test_df = pd.DataFrame(loss_data,columns=['year','value'])\n",
    "loss_data_test_df.to_csv('Loss_values/CNN_Loss_test_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_train_df = pd.DataFrame(train_data,columns=['year','value'])\n",
    "loss_data_train_df.to_csv('Loss_values/CNN_Loss_train_minmax.csv', header=False, index=False)\n",
    "\n",
    "loss_data_vald_df = pd.DataFrame(vald_data,columns=['year','value'])\n",
    "loss_data_vald_df.to_csv('Loss_values/CNN_Loss_vald_minmax.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
